{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ca64d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import copy\n",
    "from evaluation import score\n",
    "from sksurv.preprocessing import OneHotEncoder\n",
    "from sksurv.metrics import concordance_index_censored as Cindex\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09aa7756",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_intro = '../data/data_dictionary.csv'\n",
    "df_vars = pd.read_csv(data_path_intro)\n",
    "\n",
    "# create dict to specify data type\n",
    "type_mapping = {'Categorical':'category', 'Numerical':'Float64'}\n",
    "data_type_dict = {df_vars['variable'][i]:type_mapping[df_vars['type'][i]] for i in range(len(df_vars['variable']))}\n",
    "\n",
    "# read training and testing data\n",
    "data_path_train = '../data/train.csv'\n",
    "df_train = pd.read_csv(data_path_train, dtype=data_type_dict)\n",
    "\n",
    "data_path_test = '../data/test.csv'\n",
    "df_test = pd.read_csv(data_path_test, dtype=data_type_dict)\n",
    "\n",
    "data_path_sample = '../data/sample_submission.csv'\n",
    "df_sample = pd.read_csv(data_path_sample)\n",
    "\n",
    "df_train_data = df_train.drop(columns=['efs','efs_time', 'ID'])\n",
    "df_train_target = df_train[['efs', 'efs_time']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96536dd2",
   "metadata": {},
   "source": [
    "# techniques: \n",
    "<br>\n",
    "1 imputation of numerical values\n",
    "<br>\n",
    "2 age < 20 and age >= 20\n",
    "<br>\n",
    "4 2020 and 2019 combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daf7eb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding for each type category\n",
    "cat_inds = {}\n",
    "cat_var_list = []\n",
    "num_var_list = []\n",
    "num_var_exp_list = []\n",
    "for i in range(df_vars.shape[0]):\n",
    "    tp_var = df_vars['variable'][i]\n",
    "    if data_type_dict[tp_var] == 'category' and tp_var!='efs':\n",
    "        tp_cats = re.findall(r\"'(.*?)'\", df_vars['values'][i])\n",
    "        cat_inds[tp_var] = {tp_cats[i]:i+1 for i in range(len(tp_cats))}\n",
    "        cat_var_list.append(tp_var)\n",
    "    elif tp_var != 'efs_time' and tp_var!='efs':\n",
    "        num_var_list.append(tp_var)\n",
    "        num_var_exp_list.append(df_vars['description'][i])\n",
    "        \n",
    "# add additional to cat_inds\n",
    "for i in range(df_vars.shape[0]):\n",
    "    tp_var = df_vars['variable'][i]\n",
    "    if tp_var not in cat_inds and tp_var != 'donor_age' and tp_var != 'age_at_hct' and tp_var!='efs' and tp_var!='efs_time':\n",
    "        # get all values from the table\n",
    "        values_all = [tp_val for tp_val in  list(set(df_train_data[tp_var].to_list())) if tp_val is not pd.NA]\n",
    "        values_all.sort()\n",
    "        cat_inds[tp_var] = {values_all[i]:i+1 for i in range(len(values_all))}\n",
    "cat_inds['year_hct'][2020.0] = cat_inds['year_hct'][2019.0]\n",
    "\n",
    "# get categorical data\n",
    "X_all_cate = copy.deepcopy(df_train_data[list(cat_inds.keys())])\n",
    "# map to categorical indices\n",
    "for tp_var in cat_inds:\n",
    "    X_all_cate[tp_var] = X_all_cate[tp_var].map(cat_inds[tp_var])\n",
    "    X_all_cate[tp_var] = X_all_cate[tp_var].astype('category').cat.add_categories(0).fillna(0)\n",
    "# X_all_cate = X_all_cate.to_numpy(dtype=np.int16)\n",
    "X_all_num = df_train_data[[tp_var for tp_var in data_type_dict \n",
    "                           if tp_var not in cat_inds and tp_var!='efs_time' and tp_var!='efs']] # numerical data\n",
    "# X_all_num = X_all_num.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fdaab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_cate.loc[X_all_cate['hla_high_res_8']==1, 'hla_high_res_8'] = 3\n",
    "X_all_cate.loc[X_all_cate['hla_high_res_8']==2, 'hla_high_res_8'] = 3\n",
    "\n",
    "X_all_cate.loc[X_all_cate['hla_low_res_6']==1, 'hla_low_res_6'] = 2\n",
    "\n",
    "X_all_cate.loc[X_all_cate['hla_high_res_6']==1, 'hla_high_res_6'] = 3\n",
    "X_all_cate.loc[X_all_cate['hla_high_res_6']==2, 'hla_high_res_6'] = 3\n",
    "\n",
    "X_all_cate.loc[X_all_cate['hla_high_res_10']==1, 'hla_high_res_10'] = 3\n",
    "X_all_cate.loc[X_all_cate['hla_high_res_10']==2, 'hla_high_res_10'] = 3\n",
    "\n",
    "X_all_cate.loc[X_all_cate['hla_match_a_low']==1, 'hla_match_a_low'] = 2\n",
    "\n",
    "X_all_cate.loc[X_all_cate['karnofsky_score']==1, 'karnofsky_score'] = 2\n",
    "\n",
    "X_all_cate.loc[X_all_cate['hla_low_res_10']==1, 'hla_low_res_10'] = 2\n",
    "\n",
    "X_all_cate['age>20'] = (X_all_num['age_at_hct']>20).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30ed9ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill number nan with mean\n",
    "X_all_num_fill = X_all_num.to_numpy(dtype=np.float32)\n",
    "X_all_num_fill[np.isnan(X_all_num_fill)] = np.nanmean(X_all_num_fill)\n",
    "X_all_num_fill = X_all_num_fill / X_all_num_fill.mean(axis=0).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ff6552b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert values to category\n",
    "X_all_cate_int = X_all_cate.to_numpy(dtype=int)\n",
    "X_all_cate_one_hot = []\n",
    "for i in range(X_all_cate_int.shape[1]):\n",
    "    tp_col = X_all_cate_int[:, i]\n",
    "    tp_one_hot = np.zeros((tp_col.size, tp_col.max() + 1))\n",
    "    tp_one_hot[np.arange(tp_col.size), tp_col] = 1\n",
    "    X_all_cate_one_hot.append(tp_one_hot)\n",
    "    \n",
    "X_all_cate_one_hot = np.concatenate(X_all_cate_one_hot, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b1948ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_data = np.concatenate([X_all_num_fill, X_all_cate_one_hot], axis=1)\n",
    "y_all = copy.deepcopy(df_train_target)\n",
    "y_all['efs'] = y_all['efs']=='1.0'\n",
    "y_all = y_all.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0c25d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfd092c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1572c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f11eb648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split and KNN imputation\n",
    "train_prop = 0.8\n",
    "val_prop = 0.2\n",
    "train_inds = np.random.choice(len(X_all_data), size=int(train_prop*len(X_all_data)), replace=False)\n",
    "test_inds = [x for x in range(len(X_all_data)) if x not in train_inds]\n",
    "\n",
    "X_train = X_all_data[train_inds]\n",
    "X_test = X_all_data[test_inds]\n",
    "\n",
    "y_train = y_all[train_inds]\n",
    "y_test = y_all[test_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26767308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28800"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_inds) + len(train_inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948cbd20",
   "metadata": {},
   "source": [
    "# try ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e64a057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "235d1b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ebe5252",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train).float().to(device)\n",
    "X_test = torch.tensor(X_test).float().to(device)\n",
    "\n",
    "flag_train = torch.tensor([bool(i) for i in y_train[:, 0]]).float().reshape(-1,1).to(device)\n",
    "flag_test = torch.tensor([bool(i) for i in y_test[:, 0]]).float().reshape(-1,1).to(device)\n",
    "\n",
    "time_train = torch.tensor([i for i in y_train[:, 1]]).float().reshape(-1,1).to(device)\n",
    "time_test = torch.tensor([i for i in y_test[:, 1]]).float().reshape(-1,1).to(device)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(X_train, flag_train, time_train)\n",
    "val_data = torch.utils.data.TensorDataset(X_test, flag_test, time_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a961d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23040"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b8c99a2",
   "metadata": {
    "code_folding": [
     0,
     14
    ]
   },
   "outputs": [],
   "source": [
    "def R_set(x):\n",
    "    '''Create an indicator matrix of risk sets, where T_j >= T_i.\n",
    "    Note that the input data have been sorted in descending order.\n",
    "    Input:\n",
    "        x: a PyTorch tensor that the number of rows is equal to the number of samples.\n",
    "    Output:\n",
    "        indicator_matrix: an indicator matrix (which is a lower traiangular portions of matrix).\n",
    "    '''\n",
    "    n_sample = x.size(0)\n",
    "    matrix_ones = torch.ones(n_sample, n_sample)\n",
    "    indicator_matrix = torch.tril(matrix_ones)\n",
    "\n",
    "    return(indicator_matrix)\n",
    "\n",
    "def neg_par_log_likelihood(tp_pred, tp_ytime, tp_yevent):#event=0,censored\n",
    "    #ytime should be sorted with increasing order\n",
    "    '''Calculate the average Cox negative partial log-likelihood.\n",
    "    Input:\n",
    "        pred: linear predictors from trained model.\n",
    "        ytime: true survival time from load_data().\n",
    "        yevent: true censoring status from load_data().\n",
    "    Output:\n",
    "        cost: the cost that is to be minimized.\n",
    "    '''\n",
    "    # sort pred, ytime, and yevent based on ytime\n",
    "    sorted_inds = torch.argsort(tp_ytime.reshape(-1), descending=True)\n",
    "    ytime = tp_ytime[sorted_inds]\n",
    "    pred = tp_pred[sorted_inds]\n",
    "#     pred[pred>200] = 200\n",
    "    yevent = tp_yevent[sorted_inds]\n",
    "    \n",
    "    n_observed = yevent.sum(0)\n",
    "    ytime_indicator = R_set(ytime)\n",
    "    if torch.cuda.is_available():\n",
    "        ytime_indicator = ytime_indicator.cuda()\n",
    "    risk_set_sum = ytime_indicator.mm(torch.exp(pred)) \n",
    "    diff = pred - torch.log(risk_set_sum)\n",
    "    sum_diff_in_observed = torch.transpose(diff, 0, 1).mm(yevent)\n",
    "    cost = (- (sum_diff_in_observed / n_observed)).reshape((-1,))\n",
    "\n",
    "    return cost\n",
    "\n",
    "def get_c_index(pred, flag, time):\n",
    "    pred = pred.cpu().detach().numpy().reshape(-1)\n",
    "    flag = flag.cpu().detach().numpy().reshape(-1)>0\n",
    "    time = time.cpu().detach().numpy().reshape(-1)\n",
    "    return Cindex(flag, time, pred)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d47f720e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class MPLSampling(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(MPLSampling, self).__init__()\n",
    "        self.config = config\n",
    "        self.n_embed = config.n_embed\n",
    "        self.n_group = config.n_group\n",
    "        self.n_model = config.n_model\n",
    "        \n",
    "        self.latent = nn.Sequential(\n",
    "                nn.Linear(config.input_dim, config.n_embed),\n",
    "                nn.SELU(),\n",
    "                nn.Linear(config.n_embed, config.n_embed),\n",
    "                nn.SELU(),\n",
    "            )\n",
    "        \n",
    "        self.MLP = torch.nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(config.input_dim, config.n_embed),\n",
    "                nn.SELU(),\n",
    "                nn.Linear(config.n_embed, config.n_group),\n",
    "                nn.Softmax(dim=-1),\n",
    "            )\n",
    "            for _ in range(config.n_model)\n",
    "        ])\n",
    "        \n",
    "        self.out = nn.Linear(config.n_embed, 1)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        self.sampling_prop = 0.6\n",
    "        \n",
    "        self.training_data = None\n",
    "        self.is_training=True\n",
    "        \n",
    "    def initialize_training_data(self, training_data):\n",
    "        self.training_data = training_data\n",
    "        self.train_num = training_data.shape[0]\n",
    "        self.training_inds = []\n",
    "        for _ in range(self.n_model):\n",
    "            selected_train_index = np.random.choice(self.train_num, size=int(self.sampling_prop*self.train_num), replace=True)\n",
    "            self.training_inds.append(selected_train_index)\n",
    "        \n",
    "    def forward(self, inputs, targets=None):\n",
    "        \"\"\"\n",
    "        inputs: shape (batch_size, input_dim)\n",
    "        \"\"\"\n",
    "        latent_all = self.latent(self.training_data)\n",
    "        out = []\n",
    "        real_out = None\n",
    "        \n",
    "        for i in range(self.n_model):\n",
    "            selected_training_data = self.training_data[self.training_inds[i]]\n",
    "            latent_vectors = latent_all[self.training_inds[i]] # shape (n_selected, n_embed)\n",
    "            tp_mlp = self.MLP[i]\n",
    "            tp_input_gs = tp_mlp(inputs) # shape (batch_size, n_group)\n",
    "            tp_train_gs = torch.transpose(tp_mlp(selected_training_data), 0,1) # shape (n_group, n_selected)\n",
    "            \n",
    "            # construct group representations for the current model\n",
    "            tp_normalized_training_gs = tp_train_gs / tp_train_gs.sum(axis=1).unsqueeze(-1) # shape (n_group, n_selected)\n",
    "            tp_group_representation = tp_normalized_training_gs@latent_vectors # shape (n_group, n_embed)\n",
    "            \n",
    "            # get vector representation of patients\n",
    "            tp_patient_vectors = tp_input_gs@tp_group_representation# + input_latent # shape (batch_size, n_embed)\n",
    "            tp_patient_vectors = self.dropout(tp_patient_vectors)\n",
    "            tp_out = self.out(tp_patient_vectors) # shape (batch_size, 1)\n",
    "            \n",
    "            out.append(tp_out)\n",
    "            \n",
    "            if real_out is None:\n",
    "                real_out = tp_out\n",
    "            else:\n",
    "                real_out += tp_out\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            flags, times = targets\n",
    "            loss = []\n",
    "            for tp_out in out:\n",
    "                tp_loss = neg_par_log_likelihood(tp_out, times, flags)\n",
    "                loss.append(tp_loss)\n",
    "                \n",
    "        return real_out, loss\n",
    "    \n",
    "class Config():\n",
    "    def __init__(self, n_embed, n_layer, n_group, n_model, dropout):\n",
    "        self.n_embed = n_embed\n",
    "        self.n_layer = n_layer\n",
    "        \n",
    "        self.input_dim = X_train.shape[1]\n",
    "        self.n_group = n_group\n",
    "        self.n_model = n_model\n",
    "        \n",
    "        self.dropout = dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eebb650",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50ac315a",
   "metadata": {
    "code_folding": [
     28,
     33
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> start training\n",
      "current loss: 8.0706 current train C-index: 0.5049\n",
      "c index on validation set: 0.5292\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 8.0662 current train C-index: 0.5442\n",
      "c index on validation set: 0.5750\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 8.0626 current train C-index: 0.5925\n",
      "c index on validation set: 0.6221\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 8.0589 current train C-index: 0.6258\n",
      "c index on validation set: 0.6353\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 8.0552 current train C-index: 0.6392\n",
      "c index on validation set: 0.6487\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 8.0507 current train C-index: 0.6478\n",
      "c index on validation set: 0.6536\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 8.0449 current train C-index: 0.6510\n",
      "c index on validation set: 0.6565\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 8.0365 current train C-index: 0.6527\n",
      "c index on validation set: 0.6568\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 8.0291 current train C-index: 0.6535\n",
      "c index on validation set: 0.6589\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 8.0223 current train C-index: 0.6571\n",
      "c index on validation set: 0.6602\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 8.0185 current train C-index: 0.6590\n",
      "c index on validation set: 0.6624\n",
      "current learning rate: 0.000500\n",
      "\n",
      "current loss: 8.0162 current train C-index: 0.6613\n",
      "c index on validation set: 0.6637\n",
      "current learning rate: 0.000500\n",
      "\n",
      "current loss: 8.0140 current train C-index: 0.6624\n",
      "c index on validation set: 0.6656\n",
      "current learning rate: 0.000500\n",
      "\n",
      "current loss: 8.0122 current train C-index: 0.6654\n",
      "c index on validation set: 0.6652\n",
      "current learning rate: 0.000500\n",
      "\n",
      "current loss: 8.0096 current train C-index: 0.6652\n",
      "c index on validation set: 0.6650\n",
      "current learning rate: 0.000500\n",
      "\n",
      "current loss: 8.0090 current train C-index: 0.6664\n",
      "c index on validation set: 0.6663\n",
      "current learning rate: 0.000500\n",
      "\n",
      "current loss: 8.0077 current train C-index: 0.6671\n",
      "c index on validation set: 0.6664\n",
      "current learning rate: 0.000500\n",
      "\n",
      "current loss: 8.0059 current train C-index: 0.6666\n",
      "c index on validation set: 0.6675\n",
      "current learning rate: 0.000500\n",
      "\n",
      "current loss: 8.0052 current train C-index: 0.6686\n",
      "c index on validation set: 0.6663\n",
      "current learning rate: 0.000500\n",
      "\n",
      "current loss: 8.0049 current train C-index: 0.6662\n",
      "c index on validation set: 0.6656\n",
      "current learning rate: 0.000500\n",
      "\n",
      "current loss: 8.0029 current train C-index: 0.6686\n",
      "c index on validation set: 0.6673\n",
      "current learning rate: 0.000250\n",
      "\n",
      "current loss: 8.0055 current train C-index: 0.6686\n",
      "c index on validation set: 0.6675\n",
      "current learning rate: 0.000250\n",
      "\n",
      "current loss: 8.0014 current train C-index: 0.6680\n",
      "c index on validation set: 0.6654\n",
      "current learning rate: 0.000250\n",
      "\n",
      "current loss: 8.0017 current train C-index: 0.6679\n",
      "c index on validation set: 0.6686\n",
      "current learning rate: 0.000250\n",
      "\n",
      "current loss: 8.0027 current train C-index: 0.6684\n",
      "c index on validation set: 0.6677\n",
      "current learning rate: 0.000250\n",
      "\n",
      "current loss: 8.0013 current train C-index: 0.6698\n",
      "c index on validation set: 0.6666\n",
      "current learning rate: 0.000250\n",
      "\n",
      "current loss: 8.0016 current train C-index: 0.6696\n",
      "c index on validation set: 0.6655\n",
      "current learning rate: 0.000250\n",
      "\n",
      "current loss: 8.0007 current train C-index: 0.6690\n",
      "c index on validation set: 0.6664\n",
      "current learning rate: 0.000250\n",
      "\n",
      "current loss: 8.0006 current train C-index: 0.6697\n",
      "c index on validation set: 0.6671\n",
      "current learning rate: 0.000250\n",
      "\n",
      "current loss: 8.0009 current train C-index: 0.6698\n",
      "c index on validation set: 0.6670\n",
      "current learning rate: 0.000250\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     37\u001b[0m model\u001b[38;5;241m.\u001b[39mis_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m out, loss \u001b[38;5;241m=\u001b[39m model(tp_X, targets\u001b[38;5;241m=\u001b[39m(tp_flag, tp_time))\n\u001b[0;32m     40\u001b[0m loss_r \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tp_loss \u001b[38;5;129;01min\u001b[39;00m loss:\n",
      "File \u001b[1;32mC:\\shared\\groups\\jrole001\\geog0027\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\shared\\groups\\jrole001\\geog0027\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[15], line 78\u001b[0m, in \u001b[0;36mMPLSampling.forward\u001b[1;34m(self, inputs, targets)\u001b[0m\n\u001b[0;32m     76\u001b[0m     loss \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tp_out \u001b[38;5;129;01min\u001b[39;00m out:\n\u001b[1;32m---> 78\u001b[0m         tp_loss \u001b[38;5;241m=\u001b[39m neg_par_log_likelihood(tp_out, times, flags)\n\u001b[0;32m     79\u001b[0m         loss\u001b[38;5;241m.\u001b[39mappend(tp_loss)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m real_out, loss\n",
      "Cell \u001b[1;32mIn[14], line 35\u001b[0m, in \u001b[0;36mneg_par_log_likelihood\u001b[1;34m(tp_pred, tp_ytime, tp_yevent)\u001b[0m\n\u001b[0;32m     33\u001b[0m ytime_indicator \u001b[38;5;241m=\u001b[39m R_set(ytime)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m---> 35\u001b[0m     ytime_indicator \u001b[38;5;241m=\u001b[39m ytime_indicator\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     36\u001b[0m risk_set_sum \u001b[38;5;241m=\u001b[39m ytime_indicator\u001b[38;5;241m.\u001b[39mmm(torch\u001b[38;5;241m.\u001b[39mexp(pred)) \n\u001b[0;32m     37\u001b[0m diff \u001b[38;5;241m=\u001b[39m pred \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(risk_set_sum)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# other parameters\n",
    "n_embed = 32\n",
    "n_layer = 2 # number of layers\n",
    "\n",
    "n_group = 2\n",
    "n_model = 64 # number of heads\n",
    "dropout = 0.3\n",
    "\n",
    "config = Config(n_embed, n_layer, n_group, n_model, dropout)\n",
    "model = MPLSampling(config)\n",
    "model.initialize_training_data(X_train)\n",
    "model.to(device)\n",
    "batch_size = 4500\n",
    "max_iter = 200\n",
    "gamma = 0.5\n",
    "step_size = 10\n",
    "\n",
    "lr = 1e-3\n",
    "tp_lr = lr\n",
    "\n",
    "tp_ct = 0\n",
    "best_model = None\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "tp_min_loss = 999999999\n",
    "past_records = []\n",
    "lr_count  = 0\n",
    "print(\"=> start training\")\n",
    "for epoch in range(1,max_iter+1):\n",
    "    tp_rec = []\n",
    "    train_C_index = []\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=9999999, shuffle=True, drop_last=False)\n",
    "    for tp_X, tp_flag, tp_time in train_loader:\n",
    "        tp_ct += 1\n",
    "        model.zero_grad()\n",
    "        model.is_training=True\n",
    "        out, loss = model(tp_X, targets=(tp_flag, tp_time))\n",
    "        \n",
    "        loss_r = []\n",
    "        for tp_loss in loss:\n",
    "            tp_loss.backward(retain_graph=True)\n",
    "            loss_r.append(tp_loss.cpu().detach())\n",
    "        optimizer.step()\n",
    "#         loss_r = loss.cpu().detach()\n",
    "        tp_rec.extend(loss_r)\n",
    "        train_C_index.append(get_c_index(out, tp_flag, tp_time))\n",
    "#         print(loss_r)\n",
    "    tp_epoch_loss = float(sum(tp_rec)/len(tp_rec))\n",
    "    tp_epoch_C = float(sum(train_C_index)/len(train_C_index))\n",
    "    print(\"current loss: {:.4f}\".format(tp_epoch_loss) + \" current train C-index: {:.4f}\".format(tp_epoch_C))\n",
    "    if tp_min_loss>tp_epoch_loss: # if the current loss is less than the min_loss record\n",
    "        tp_min_loss = float(tp_epoch_loss)\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "    # calculate the c-index on validation set\n",
    "    with torch.no_grad():\n",
    "        for tp_X, tp_flag, tp_time in val_loader:\n",
    "            model.is_training=False\n",
    "            out, loss = model(tp_X, targets=(tp_flag, tp_time))\n",
    "            tp_c_index = get_c_index(out, tp_flag, tp_time)\n",
    "            print(\"c index on validation set: {:.4f}\".format(tp_c_index))\n",
    "            print('current learning rate: {:.6f}'.format(tp_lr))\n",
    "    print()\n",
    "    \n",
    "    scheduler.step()\n",
    "    if epoch%step_size==0:\n",
    "        tp_lr *= gamma\n",
    "    past_records.append(tp_epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5d2f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6186e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31ed3fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
