{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ca64d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import copy\n",
    "from evaluation import score\n",
    "from sksurv.preprocessing import OneHotEncoder\n",
    "from sksurv.metrics import concordance_index_censored as Cindex\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09aa7756",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_intro = '../data/data_dictionary.csv'\n",
    "df_vars = pd.read_csv(data_path_intro)\n",
    "\n",
    "# create dict to specify data type\n",
    "type_mapping = {'Categorical':'category', 'Numerical':'Float64'}\n",
    "data_type_dict = {df_vars['variable'][i]:type_mapping[df_vars['type'][i]] for i in range(len(df_vars['variable']))}\n",
    "\n",
    "# read training and testing data\n",
    "data_path_train = '../data/train.csv'\n",
    "df_train = pd.read_csv(data_path_train, dtype=data_type_dict)\n",
    "\n",
    "data_path_test = '../data/test.csv'\n",
    "df_test = pd.read_csv(data_path_test, dtype=data_type_dict)\n",
    "\n",
    "data_path_sample = '../data/sample_submission.csv'\n",
    "df_sample = pd.read_csv(data_path_sample)\n",
    "\n",
    "df_train_data = df_train.drop(columns=['efs','efs_time', 'ID'])\n",
    "df_train_target = df_train[['efs', 'efs_time']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96536dd2",
   "metadata": {},
   "source": [
    "# techniques: \n",
    "<br>\n",
    "1 imputation of numerical values\n",
    "<br>\n",
    "2 age < 20 and age >= 20\n",
    "<br>\n",
    "4 2020 and 2019 combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20a7b082",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_all = OneHotEncoder().fit_transform(df_train_data)\n",
    "X_all['age>20'] = (X_all['age_at_hct']>20).astype(float)\n",
    "X_all['year_hct'] = X_all['year_hct']-2000\n",
    "X_all.loc[X_all['year_hct']==20,'year_hct'] = 19\n",
    "y_all = copy.deepcopy(df_train_target)\n",
    "y_all['efs'] = y_all['efs']=='1.0'\n",
    "y_all = y_all.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "719624c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill nan with zeros for categorical and with mean for numerical\n",
    "X_all_data = []\n",
    "for tp_col_name in X_all.columns.values:\n",
    "    if '=' in tp_col_name:\n",
    "        tp_col_data = np.nan_to_num(X_all[tp_col_name], nan=0)\n",
    "        X_all_data.append(tp_col_data)\n",
    "        if np.isnan(tp_col_data).sum()>0:\n",
    "            print('???')\n",
    "            break\n",
    "    else:\n",
    "        tp_data = copy.deepcopy(X_all[tp_col_name].to_numpy(dtype=np.float32))\n",
    "        tp_data[np.isnan(tp_data)] = np.nanmean(tp_data)\n",
    "        X_all_data.append(tp_data/tp_data.mean())\n",
    "        if np.isnan(tp_data).sum()>0:\n",
    "            print('???')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4533156",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_data = np.array(X_all_data).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08de7675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(X_all_data).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3982f4e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 1.15410984,\n",
       "        0.        ],\n",
       "       [0.        , 1.        , 0.        , ..., 1.        , 1.15410984,\n",
       "        1.33847654],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 1.15410984,\n",
       "        1.33847654],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 1.15410984,\n",
       "        1.33847654],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.57705492,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 1.        , 1.15410984,\n",
       "        0.        ]], shape=(28800, 149))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4019b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f11eb648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split and KNN imputation\n",
    "train_prop = 0.8\n",
    "val_prop = 0.2\n",
    "train_inds = np.random.choice(len(X_all_data), size=int(train_prop*len(X_all_data)), replace=False)\n",
    "test_inds = [x for x in range(len(X_all_data)) if x not in train_inds]\n",
    "\n",
    "X_train = X_all_data[train_inds]\n",
    "X_test = X_all_data[test_inds]\n",
    "\n",
    "y_train = y_all[train_inds]\n",
    "y_test = y_all[test_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26767308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28800"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_inds) + len(train_inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948cbd20",
   "metadata": {},
   "source": [
    "# try ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e64a057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "235d1b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ebe5252",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train).float().to(device)\n",
    "X_test = torch.tensor(X_test).float().to(device)\n",
    "\n",
    "flag_train = torch.tensor([bool(i) for i in y_train[:, 0]]).float().reshape(-1,1).to(device)\n",
    "flag_test = torch.tensor([bool(i) for i in y_test[:, 0]]).float().reshape(-1,1).to(device)\n",
    "\n",
    "time_train = torch.tensor([i for i in y_train[:, 1]]).float().reshape(-1,1).to(device)\n",
    "time_test = torch.tensor([i for i in y_test[:, 1]]).float().reshape(-1,1).to(device)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(X_train, flag_train, time_train)\n",
    "val_data = torch.utils.data.TensorDataset(X_test, flag_test, time_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a961d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23040"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b8c99a2",
   "metadata": {
    "code_folding": [
     0,
     14
    ]
   },
   "outputs": [],
   "source": [
    "def R_set(x):\n",
    "    '''Create an indicator matrix of risk sets, where T_j >= T_i.\n",
    "    Note that the input data have been sorted in descending order.\n",
    "    Input:\n",
    "        x: a PyTorch tensor that the number of rows is equal to the number of samples.\n",
    "    Output:\n",
    "        indicator_matrix: an indicator matrix (which is a lower traiangular portions of matrix).\n",
    "    '''\n",
    "    n_sample = x.size(0)\n",
    "    matrix_ones = torch.ones(n_sample, n_sample)\n",
    "    indicator_matrix = torch.tril(matrix_ones)\n",
    "\n",
    "    return(indicator_matrix)\n",
    "\n",
    "def neg_par_log_likelihood(tp_pred, tp_ytime, tp_yevent):#event=0,censored\n",
    "    #ytime should be sorted with increasing order\n",
    "    '''Calculate the average Cox negative partial log-likelihood.\n",
    "    Input:\n",
    "        pred: linear predictors from trained model.\n",
    "        ytime: true survival time from load_data().\n",
    "        yevent: true censoring status from load_data().\n",
    "    Output:\n",
    "        cost: the cost that is to be minimized.\n",
    "    '''\n",
    "    # sort pred, ytime, and yevent based on ytime\n",
    "    sorted_inds = torch.argsort(tp_ytime.reshape(-1), descending=True)\n",
    "    ytime = tp_ytime[sorted_inds]\n",
    "    pred = tp_pred[sorted_inds]\n",
    "#     pred[pred>200] = 200\n",
    "    yevent = tp_yevent[sorted_inds]\n",
    "    \n",
    "    n_observed = yevent.sum(0)\n",
    "    ytime_indicator = R_set(ytime)\n",
    "    if torch.cuda.is_available():\n",
    "        ytime_indicator = ytime_indicator.cuda()\n",
    "    risk_set_sum = ytime_indicator.mm(torch.exp(pred)) \n",
    "    diff = pred - torch.log(risk_set_sum)\n",
    "    sum_diff_in_observed = torch.transpose(diff, 0, 1).mm(yevent)\n",
    "    cost = (- (sum_diff_in_observed / n_observed)).reshape((-1,))\n",
    "\n",
    "    return cost\n",
    "\n",
    "def get_c_index(pred, flag, time):\n",
    "    pred = pred.cpu().detach().numpy().reshape(-1)\n",
    "    flag = flag.cpu().detach().numpy().reshape(-1)>0\n",
    "    time = time.cpu().detach().numpy().reshape(-1)\n",
    "    return Cindex(flag, time, pred)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d47f720e",
   "metadata": {
    "code_folding": [
     65
    ]
   },
   "outputs": [],
   "source": [
    "class MPLSampling(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(MPLSampling, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.latent = nn.Linear(config.input_dim, config.n_embed)\n",
    "        \n",
    "        self.MLP = torch.nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(config.input_dim, config.n_embed),\n",
    "                nn.SELU(),\n",
    "                nn.Linear(config.n_embed, config.n_group),\n",
    "                nn.Softmax(dim=-1),\n",
    "            )\n",
    "            for _ in range(config.n_model)\n",
    "        ])\n",
    "        \n",
    "        self.out = nn.Linear(config.n_embed, 1)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        self.sampling_prop = 0.6\n",
    "        \n",
    "        self.training_data = None\n",
    "        self.is_training=True\n",
    "        \n",
    "    def initialize_training_data(self, training_data):\n",
    "        self.training_data = training_data\n",
    "        self.train_num = training_data.shape[0]\n",
    "        \n",
    "    def forward(self, inputs, targets=None):\n",
    "        \"\"\"\n",
    "        inputs: shape (batch_size, input_dim)\n",
    "        \"\"\"\n",
    "        # selecting random data\n",
    "        if model.is_training:\n",
    "            selected_train_index = np.random.choice(self.train_num, size=int(self.sampling_prop*self.train_num), replace=False)\n",
    "            selected_training_data = self.training_data[selected_train_index]\n",
    "        else:\n",
    "            selected_training_data = self.training_data\n",
    "        \n",
    "        latent_vectors = self.latent(selected_training_data) # shape (n_selected, n_embed)\n",
    "        out = []\n",
    "        real_out = None\n",
    "        for tp_mlp in self.MLP:\n",
    "            tp_input_gs = tp_mlp(inputs) # shape (batch_size, n_group)\n",
    "            tp_train_gs = torch.transpose(tp_mlp(selected_training_data), 0,1) # shape (n_group, n_selected)\n",
    "            \n",
    "            # construct group representations for the current model\n",
    "            tp_normalized_training_gs = tp_train_gs / tp_train_gs.sum(axis=1).unsqueeze(-1) # shape (n_group, n_selected)\n",
    "            tp_group_representation = tp_normalized_training_gs@latent_vectors # shape (n_group, n_embed)\n",
    "            \n",
    "            # get vector representation of patients\n",
    "            tp_patient_vectors = tp_input_gs@tp_group_representation # shape (batch_size, n_embed)\n",
    "            tp_patient_vectors = self.dropout(tp_patient_vectors)\n",
    "            tp_out = self.out(tp_patient_vectors) # shape (batch_size, 1)\n",
    "            \n",
    "            out.append(tp_out)\n",
    "            \n",
    "            if real_out is None:\n",
    "                real_out = tp_out\n",
    "            else:\n",
    "                real_out += tp_out\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            flags, times = targets\n",
    "            loss = []\n",
    "            for tp_out in out:\n",
    "                tp_loss = neg_par_log_likelihood(tp_out, times, flags)\n",
    "                loss.append(tp_loss)\n",
    "                \n",
    "        return real_out, loss\n",
    "    \n",
    "class Config():\n",
    "    def __init__(self, n_embed, n_layer, n_group, n_model, dropout):\n",
    "        self.n_embed = n_embed\n",
    "        self.n_layer = n_layer\n",
    "        \n",
    "        self.input_dim = X_train.shape[1]\n",
    "        self.n_group = n_group\n",
    "        self.n_model = n_model\n",
    "        \n",
    "        self.dropout = dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50ac315a",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> start training\n",
      "current loss: 6.5888 current train C-index: 0.5515\n",
      "c index on validation set: 0.6209\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 6.5796 current train C-index: 0.6307\n",
      "c index on validation set: 0.6409\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 6.5700 current train C-index: 0.6400\n",
      "c index on validation set: 0.6454\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 6.5583 current train C-index: 0.6447\n",
      "c index on validation set: 0.6471\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 6.5529 current train C-index: 0.6515\n",
      "c index on validation set: 0.6517\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 6.5481 current train C-index: 0.6546\n",
      "c index on validation set: 0.6558\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 6.5456 current train C-index: 0.6589\n",
      "c index on validation set: 0.6572\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 6.5449 current train C-index: 0.6620\n",
      "c index on validation set: 0.6581\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 6.5437 current train C-index: 0.6614\n",
      "c index on validation set: 0.6594\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 6.5442 current train C-index: 0.6638\n",
      "c index on validation set: 0.6589\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 6.5417 current train C-index: 0.6654\n",
      "c index on validation set: 0.6564\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 6.5405 current train C-index: 0.6659\n",
      "c index on validation set: 0.6588\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 6.5402 current train C-index: 0.6658\n",
      "c index on validation set: 0.6608\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 6.5396 current train C-index: 0.6678\n",
      "c index on validation set: 0.6598\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 6.5392 current train C-index: 0.6674\n",
      "c index on validation set: 0.6598\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 6.5390 current train C-index: 0.6689\n",
      "c index on validation set: 0.6603\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 6.5377 current train C-index: 0.6674\n",
      "c index on validation set: 0.6581\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 6.5377 current train C-index: 0.6688\n",
      "c index on validation set: 0.6605\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 6.5370 current train C-index: 0.6680\n",
      "c index on validation set: 0.6602\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 6.5356 current train C-index: 0.6694\n",
      "c index on validation set: 0.6609\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 6.5379 current train C-index: 0.6688\n",
      "c index on validation set: 0.6604\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 6.5360 current train C-index: 0.6687\n",
      "c index on validation set: 0.6605\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 6.5367 current train C-index: 0.6701\n",
      "c index on validation set: 0.6616\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 6.5360 current train C-index: 0.6680\n",
      "c index on validation set: 0.6584\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 6.5355 current train C-index: 0.6704\n",
      "c index on validation set: 0.6608\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 6.5351 current train C-index: 0.6696\n",
      "c index on validation set: 0.6618\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 6.5349 current train C-index: 0.6700\n",
      "c index on validation set: 0.6609\n",
      "current learning rate: 0.001000\n",
      "\n",
      "current loss: 6.5345 current train C-index: 0.6698\n",
      "c index on validation set: 0.6614\n",
      "current learning rate: 0.001000\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input estimate contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m#         loss_r = loss.cpu().detach()\u001b[39;00m\n\u001b[0;32m     46\u001b[0m         tp_rec\u001b[38;5;241m.\u001b[39mextend(loss_r)\n\u001b[1;32m---> 47\u001b[0m         train_C_index\u001b[38;5;241m.\u001b[39mappend(get_c_index(out, tp_flag, tp_time))\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m#         print(loss_r)\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     tp_epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28msum\u001b[39m(tp_rec)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(tp_rec))\n",
      "Cell \u001b[1;32mIn[14], line 47\u001b[0m, in \u001b[0;36mget_c_index\u001b[1;34m(pred, flag, time)\u001b[0m\n\u001b[0;32m     45\u001b[0m flag \u001b[38;5;241m=\u001b[39m flag\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     46\u001b[0m time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Cindex(flag, time, pred)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mC:\\shared\\groups\\jrole001\\geog0027\\envs\\pytorch\\Lib\\site-packages\\sksurv\\metrics.py:216\u001b[0m, in \u001b[0;36mconcordance_index_censored\u001b[1;34m(event_indicator, event_time, estimate, tied_tol)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcordance_index_censored\u001b[39m(event_indicator, event_time, estimate, tied_tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m):\n\u001b[0;32m    152\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Concordance index for right-censored data\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m    The concordance index is defined as the proportion of all comparable pairs\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;124;03m           Statistics in Medicine, 15(4), 361-87, 1996.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m     event_indicator, event_time, estimate \u001b[38;5;241m=\u001b[39m _check_inputs(event_indicator, event_time, estimate)\n\u001b[0;32m    218\u001b[0m     w \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones_like(estimate)\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _estimate_concordance_index(event_indicator, event_time, estimate, w, tied_tol)\n",
      "File \u001b[1;32mC:\\shared\\groups\\jrole001\\geog0027\\envs\\pytorch\\Lib\\site-packages\\sksurv\\metrics.py:47\u001b[0m, in \u001b[0;36m_check_inputs\u001b[1;34m(event_indicator, event_time, estimate)\u001b[0m\n\u001b[0;32m     45\u001b[0m event_indicator \u001b[38;5;241m=\u001b[39m check_array(event_indicator, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent_indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m event_time \u001b[38;5;241m=\u001b[39m check_array(event_time, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent_time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 47\u001b[0m estimate \u001b[38;5;241m=\u001b[39m _check_estimate_1d(estimate, event_time)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(event_indicator\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mbool_):\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly boolean arrays are supported as class labels for survival analysis, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_indicator\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     52\u001b[0m     )\n",
      "File \u001b[1;32mC:\\shared\\groups\\jrole001\\geog0027\\envs\\pytorch\\Lib\\site-packages\\sksurv\\metrics.py:36\u001b[0m, in \u001b[0;36m_check_estimate_1d\u001b[1;34m(estimate, test_time)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_estimate_1d\u001b[39m(estimate, test_time):\n\u001b[1;32m---> 36\u001b[0m     estimate \u001b[38;5;241m=\u001b[39m check_array(estimate, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m estimate\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 1D array, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimate\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\shared\\groups\\jrole001\\geog0027\\envs\\pytorch\\Lib\\site-packages\\sklearn\\utils\\validation.py:1049\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1043\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1044\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1045\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1046\u001b[0m     )\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1049\u001b[0m     _assert_all_finite(\n\u001b[0;32m   1050\u001b[0m         array,\n\u001b[0;32m   1051\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m   1052\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m   1053\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1054\u001b[0m     )\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1057\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1058\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mC:\\shared\\groups\\jrole001\\geog0027\\envs\\pytorch\\Lib\\site-packages\\sklearn\\utils\\validation.py:126\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    127\u001b[0m     X,\n\u001b[0;32m    128\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[0;32m    129\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[0;32m    130\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[0;32m    131\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    132\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    133\u001b[0m )\n",
      "File \u001b[1;32mC:\\shared\\groups\\jrole001\\geog0027\\envs\\pytorch\\Lib\\site-packages\\sklearn\\utils\\validation.py:175\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    174\u001b[0m     )\n\u001b[1;32m--> 175\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input estimate contains NaN."
     ]
    }
   ],
   "source": [
    "# other parameters\n",
    "n_embed = 32\n",
    "n_layer = 2 # number of layers\n",
    "\n",
    "n_group = 4\n",
    "n_model = 64 # number of heads\n",
    "dropout = 0.3\n",
    "\n",
    "config = Config(n_embed, n_layer, n_group, n_model, dropout)\n",
    "model = MPLSampling(config)\n",
    "model.initialize_training_data(X_train)\n",
    "model.to(device)\n",
    "batch_size = 1024\n",
    "max_iter = 200\n",
    "gamma = 0.8\n",
    "step_size = 40\n",
    "\n",
    "lr = 1e-3\n",
    "tp_lr = lr\n",
    "\n",
    "tp_ct = 0\n",
    "best_model = None\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "tp_min_loss = 999999999\n",
    "past_records = []\n",
    "lr_count  = 0\n",
    "print(\"=> start training\")\n",
    "for epoch in range(1,max_iter+1):\n",
    "    tp_rec = []\n",
    "    train_C_index = []\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=9999999, shuffle=True, drop_last=False)\n",
    "    for tp_X, tp_flag, tp_time in train_loader:\n",
    "        tp_ct += 1\n",
    "        model.zero_grad()\n",
    "        model.is_training=True\n",
    "        out, loss = model(tp_X, targets=(tp_flag, tp_time))\n",
    "        \n",
    "        loss_r = []\n",
    "        for tp_loss in loss:\n",
    "            tp_loss.backward(retain_graph=True)\n",
    "            loss_r.append(tp_loss.cpu().detach())\n",
    "        optimizer.step()\n",
    "#         loss_r = loss.cpu().detach()\n",
    "        tp_rec.extend(loss_r)\n",
    "        train_C_index.append(get_c_index(out, tp_flag, tp_time))\n",
    "#         print(loss_r)\n",
    "    tp_epoch_loss = float(sum(tp_rec)/len(tp_rec))\n",
    "    tp_epoch_C = float(sum(train_C_index)/len(train_C_index))\n",
    "    print(\"current loss: {:.4f}\".format(tp_epoch_loss) + \" current train C-index: {:.4f}\".format(tp_epoch_C))\n",
    "    if tp_min_loss>tp_epoch_loss: # if the current loss is less than the min_loss record\n",
    "        tp_min_loss = float(tp_epoch_loss)\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "    # calculate the c-index on validation set\n",
    "    with torch.no_grad():\n",
    "        for tp_X, tp_flag, tp_time in val_loader:\n",
    "            model.is_training=False\n",
    "            out, loss = model(tp_X, targets=(tp_flag, tp_time))\n",
    "            tp_c_index = get_c_index(out, tp_flag, tp_time)\n",
    "            print(\"c index on validation set: {:.4f}\".format(tp_c_index))\n",
    "            print('current learning rate: {:.6f}'.format(tp_lr))\n",
    "    print()\n",
    "    \n",
    "    scheduler.step()\n",
    "    if epoch%step_size==0:\n",
    "        tp_lr *= gamma\n",
    "    past_records.append(tp_epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5d2f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0753d468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
